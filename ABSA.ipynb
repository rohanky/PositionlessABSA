{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Try1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB2p1Z44B6_u"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from textblob import TextBlob\n",
        "tokenizerR = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeT0PmG-CCx5"
      },
      "source": [
        "#Read the data and prepocess\n",
        "df = pd.read_csv('abs.csv')\n",
        "target = df.iloc[:,0:1].values\n",
        "text = df.iloc[:,1:2].values\n",
        "textMask = df.iloc[:,2:3].values\n",
        "y = df.iloc[:,3:4].values\n",
        "\n",
        "y = np.reshape(y, len(y))\n",
        "\n",
        "\n",
        "\n",
        "dflex = pd.read_csv('lexicon.csv')\n",
        "posLex = dflex.iloc[:,1:2].values\n",
        "negLex = dflex.iloc[:,0:1].values\n",
        "\n",
        "\n",
        "\n",
        "def prepreocess(data):\n",
        "\n",
        "    input_data=[]\n",
        "    vocab   = []\n",
        "    for i in data:\n",
        "        for j in i:\n",
        "            \n",
        "            j = j.lower()\n",
        "            j = j.replace(\"\\n\", \"\")\n",
        "            j = j.replace('n\\'t', 'not')\n",
        "            j = j.replace('\\'ve', 'have')\n",
        "            j = j.replace('\\'ll', 'will')\n",
        "            j = j.replace('\\'re', 'are')\n",
        "            j = j.replace('\\'m', 'am')\n",
        "            j = j.replace('/', ' / ')\n",
        "            j = j.replace('-', ' ')\n",
        "            j = j.replace('!', ' ')\n",
        "            j = j.replace('?', ' ')\n",
        "            j = j.replace('+', ' ')\n",
        "            j = j.replace('*', ' ')\n",
        "            while \"  \" in j:\n",
        "                j = j.replace('  ', ' ')\n",
        "            while \",,\" in j:\n",
        "                j = j.replace(',,', ',')\n",
        "            j = j.strip()\n",
        "            j = j.strip('.')\n",
        "            j = j.strip()\n",
        "            temp1 = tokenizerR.tokenize(j)\n",
        "\n",
        "\n",
        "            temp2 = [x for x in temp1 if not x.isdigit()]\n",
        "            #temp3 = [w for w in temp2 if not w in stop_words]\n",
        "            input_data.append(temp2)\n",
        "            for k in temp2:\n",
        "                vocab.append(k)\n",
        "    return vocab, input_data\n",
        "\n",
        "vocab, input_text = prepreocess(text)\n",
        "temp , input_target = prepreocess(textMask)\n",
        "\n",
        "def prepreocess_lexicon(data):\n",
        "    lexVocab = []\n",
        "    for i in data:\n",
        "        for j in i:\n",
        "            lexVocab.append(j)\n",
        "\n",
        "    return lexVocab\n",
        "\n",
        "\n",
        "pos = prepreocess_lexicon(posLex)\n",
        "neg = prepreocess_lexicon(negLex)\n",
        "\n",
        "def lexicon_based(data1):\n",
        "    for i in data1:\n",
        "        for j in range(len(i)):\n",
        "            if i[j] in pos[:]:\n",
        "                i[j] = 'positive' \n",
        "            if i[j] in neg[:]:\n",
        "                i[j] = 'negative'\n",
        "    return data1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnttC_bDCLpX"
      },
      "source": [
        "#Prerpocess\n",
        "lexicon_input = lexicon_based(input_text)\n",
        "lexicon_target = lexicon_based(input_target)\n",
        "\n",
        "#Label Encoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "categorical_labels = to_categorical(y, num_classes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHRp35gcCQhI"
      },
      "source": [
        "#Tokenize and Split datasets\n",
        "from keras.preprocessing import text, sequence\n",
        "def make_df(train_path,train_path_mask, max_features, maxlen, as_max_len, label):\n",
        "    tokenizer1 = text.Tokenizer(num_words=max_features)\n",
        "    tokenizer1.fit_on_texts(train_path)\n",
        "    tokenizer1.fit_on_texts(train_path_mask)\n",
        "    list_tokenized_train = tokenizer1.texts_to_sequences(train_path)\n",
        "    print(tokenizer1)\n",
        "    data = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post')\n",
        "    list_tokenized_train1 = tokenizer1.texts_to_sequences(train_path_mask)\n",
        "    data1 = sequence.pad_sequences(list_tokenized_train1, maxlen=as_max_len, padding='post')\n",
        "    X_t = data[:3608,:]\n",
        "    X_te = data[3608:,:]\n",
        "    ytr = label[0:3608,:]\n",
        "    yte = label[3608:,:]\n",
        "    X_t_mask = data1[:3608,:]\n",
        "    X_te_mask = data1[3608:,:]\n",
        "\n",
        "    word_index = tokenizer1.word_index\n",
        "    print(word_index)\n",
        "    return X_t, X_te, X_t_mask, X_te_mask, ytr, yte, word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqNtLO1oefww"
      },
      "source": [
        "#Encode the Glove embedding into Input\n",
        "import numpy as np\n",
        "def make_glovevec(glovepath, max_features, embed_size, word_index, veclen=300):\n",
        "    embeddings_index = {}\n",
        "    f = open(glovepath)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = ' '.join(values[:-300])\n",
        "        coefs = np.asarray(values[-300:], dtype='float32')\n",
        "        embeddings_index[word] = coefs.reshape(-1)\n",
        "    f.close()\n",
        "\n",
        "    nb_words = min(max_features, len(word_index))\n",
        "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEpI36npea2z"
      },
      "source": [
        "#Download Glove Embedding\n",
        "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoCoZKDuecqK"
      },
      "source": [
        "\n",
        "#unzip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHKpb-JrCUDf"
      },
      "source": [
        "\n",
        "#Call the required  function\n",
        "max_features = 3600\n",
        "maxlen = 85\n",
        "embed_size =300\n",
        "as_max_len = 85\n",
        "    \n",
        "xtr, xte, xtr_mask, xte_mask, ytr,yte,  word_index = make_df(lexicon_input,lexicon_target, max_features, maxlen,as_max_len, categorical_labels)\n",
        "embedding_vector = make_glovevec(\"glove.42B.300d.txt\", max_features, embed_size, word_index)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWcdElq_dE4P"
      },
      "source": [
        "from keras.layers import Layer, Input, Embedding, Bidirectional, LSTM, Concatenate, Dense\n",
        "import keras.backend as K\n",
        "from keras.layers import *\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmrgy2h6Casw"
      },
      "source": [
        "# Attention Layer\n",
        "class Attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.attention_dim = 75\n",
        "        super(Attention,self).__init__(**kwargs)\n",
        "\n",
        "\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\", trainable = True)\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(self.attention_dim,),initializer=\"normal\", trainable = True)\n",
        "        self.u=self.add_weight(name=\"u_bias\",shape=(self.attention_dim,1),initializer=\"normal\", trainable = True)        \n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def call(self,x, mask=None):\n",
        "        et= K.tanh(K.dot(x,self.W)+self.b)\n",
        "        ait = K.dot(et, self.u)\n",
        "        ait = K.squeeze(ait, -1)\t\t\n",
        "        ait = K.exp(ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "            \n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(Attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH-fLRWUCctR"
      },
      "source": [
        "#Main BiRU based Model\n",
        "word_attention_model = None\n",
        "def build_model(VOCABULARY_SIZE, MAX_SENTENCE_LENGTH, ASP_MAX_LEN, embedding_weights, n_classes=3, embedding_dim=300):\n",
        "\n",
        "    l2_reg = regularizers.l2(0.001)\n",
        "    sentence_in = Input(shape=(MAX_SENTENCE_LENGTH,), name=\"input_1\")\n",
        "    aspect_in = Input(shape=(ASP_MAX_LEN,), name=\"input_2\")\n",
        "    embedding_trainable = False\n",
        "    embedded_word_seq = Embedding(VOCABULARY_SIZE,embedding_dim,input_length=MAX_SENTENCE_LENGTH,weights=[embedding_weights],trainable=False,mask_zero=False)(sentence_in)\n",
        "\n",
        "    word_encoder = Bidirectional(GRU(300,return_sequences=True, dropout=0.5))(embedded_word_seq)\n",
        "    \n",
        "    #dense_transform_word = Dense(100, activation='relu', name='dense_transform_word', kernel_regularizer=l2_reg)(word_encoder)\n",
        "\n",
        "    embedded_aspect = Embedding(VOCABULARY_SIZE,embedding_dim,input_length=ASP_MAX_LEN,weights=[embedding_weights],trainable=False,mask_zero=False)(aspect_in)\n",
        "\n",
        "    aspect_encoder = Bidirectional(GRU(300, return_sequences=True, dropout=0.5))(embedded_aspect)\n",
        "\n",
        "    #dropout1 = Dropout(0.5)(aspect_encoder)\n",
        "\n",
        "    #dense_transform_word1 = Dense(100, activation='relu', name='dense_transform_word1', kernel_regularizer=l2_reg)(aspect_encoder)\n",
        "\n",
        "    attention_weighted_text1 = Attention(name=\"sentence_attention1\")(aspect_encoder)\n",
        "\n",
        "\n",
        "    attention_weighted_text = Attention(name=\"sentence_attention\")(word_encoder)\n",
        "  \n",
        "    attention_fusion = Concatenate()([attention_weighted_text, attention_weighted_text1])\n",
        "\n",
        "  \n",
        "\n",
        "    prediction = Dense(n_classes, activation='softmax')(attention_fusion)\n",
        "\n",
        "    model = Model([sentence_in, aspect_in], prediction)\n",
        "\n",
        "\n",
        "    optimizer=Adam(lr=0.001, decay=0.0001)\n",
        "\n",
        "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9LJ51b9CgDZ"
      },
      "source": [
        "#Run Experiments\n",
        "K.clear_session()\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
        "model = build_model(max_features, as_max_len, as_max_len, embedding_vector, 3, embed_size)\n",
        "\n",
        "#filepath=\"weights.best.hdf5\"\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "#model.fit([xtr, xtr_mask], ytr, validation_data=([xte,xte_mask], yte), batch_size=64, epochs=100,callbacks=[checkpoint], verbose=1)\n",
        "#csv_logger = CSVLogger(\"res15(0.1).csv\", append=True)\n",
        "model.fit([xtr, xtr_mask], ytr, validation_data=([xte,xte_mask], yte), batch_size=64, epochs=50, verbose=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CSoAx8sBVrY"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.utils import CustomObjectScope\n",
        "model = load_model(\"weights.best.hdf5\",custom_objects={'Attention': Attention})\n",
        "model.summary()\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate([xte, xte_mask], yte, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "ypred = model.predict([xte, xte_mask])\n",
        "y_pred_class = ypred.argmax(axis=-1)\n",
        "ytrue = yte.argmax(axis=-1)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "F1 = f1_score(ytrue, y_pred_class, average='macro')\n",
        "print(F1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAyBKBB9ZN4"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(ytrue, y_pred_class)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzM1_JpQ-fys"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax, cmap=\"Blues\", fmt='g'); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "#ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['neg', 'neu', 'pos']); ax.yaxis.set_ticklabels(['neg', 'neu', 'pos']);\n",
        "plt.savefig('confusion_matrix'+' res16'+'.pdf', bbox_inches = \"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RKldVtLCUUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54e29b1f-4030-4df2-ec91-53a65692c1fa"
      },
      "source": [
        "model = load_model(\"weights.best.hdf5\",custom_objects={'Attention': Attention})\n",
        "model = Model(inputs=model.input,outputs=[model.output, model.get_layer('bidirectional').output])\n",
        "attn = model.get_layer('sentence_attention').output\n",
        "encoded_input_text = [xte[0:1:,:],xte_mask[0:1,:]]\n",
        "outputs_bfr_attn = model.predict(encoded_input_text)\n",
        "weight = outputs_bfr_attn[1]\n",
        "weight.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 80, 600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV4yD0_fCV9O"
      },
      "source": [
        "model = load_model(\"weights.best.hdf5\",custom_objects={'Attention': Attention})\n",
        "encoded_input_text = [xte[179:180:,:],xte_mask[179:180,:]]\n",
        "layer_weights = model.layers[6].get_weights()\n",
        "new_model = Model(inputs=model.input, outputs=model.layers[4].output)\n",
        "output_before_att = new_model.predict(encoded_input_text) #extract layer output\n",
        "eij = np.tanh(np.dot(output_before_att, layer_weights[0]) + layer_weights[1]) #Eq.(5) in the paper\n",
        "eij = np.dot(eij, layer_weights[2]) #Eq.(6)\n",
        "\n",
        "eij = eij.reshape((eij.shape[0], eij.shape[1])) # reshape the vector\n",
        "\n",
        "ai = np.exp(eij) #Eq.(6)\n",
        "\n",
        "weights = ai / np.sum(ai) # Eq.(6)\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxjxWSf9sgF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26
        },
        "outputId": "4ab1200b-8583-4a2b-b448-8f9858c9a68f"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "from string import Template\n",
        "import json\n",
        "\n",
        "def display_attention(sentence,attention_weights,scale=0,offset=18,style=1):\n",
        "\n",
        "\tif( len(sentence)!=len(attention_weights)):\n",
        "\t\traise Exception(\"Number of tokens \"+ str(len(sentence)) + \" not equal to attention list length \" + str(len(attention_weights)))\n",
        "\n",
        "\tfor _ in attention_weights:\n",
        "\t\tif _ >1.0 or _ < 0.0:\n",
        "\t\t\traise Exception(\"invalid value \" + str(_) + \"in attention_weights\")\n",
        "\n",
        "\thtml_template = Template('''\n",
        "\t<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
        "\t<style>$css_text</style>\n",
        "\t<script>$js_text</script>\n",
        "\t<div id = 'text' style=\"margin-left:50px;\"></div>\n",
        "\t''')\n",
        "\n",
        "\tcss_text = '''\n",
        "\thtml, body {\n",
        "\tmargin: 0;\n",
        "\tpadding: 0;\n",
        "\t}\n",
        "\t.tooltip {\n",
        "\tposition: relative;\n",
        "\tdisplay: inline-block;\n",
        "\tborder-bottom: 1px dotted black;\n",
        "\t}\n",
        "\t.tooltip:hover .tooltiptext {\n",
        "\tvisibility: visible;\n",
        "\t}\n",
        "\t.tooltip .tooltiptext {\n",
        "\tvisibility: hidden;\n",
        "\tfont-size:15px;\n",
        "\twidth: 60px;\n",
        "\tbackground-color: black;\n",
        "\tcolor: #fff;\n",
        "\ttext-align: center;\n",
        "\tborder-radius: 6px;\n",
        "\tpadding: 1px 0;\n",
        "\tposition: absolute;\n",
        "\tleft: 50%;\n",
        "\tmargin-left: -60px;\n",
        "\t}\n",
        "\t'''\n",
        "\n",
        "\tjs_template = Template('''\n",
        "\tvar dataset =[[\"This\",0.4],[\"is\",0.3],[\"a\",0.2],[\"text\",0.4]]\n",
        "\tvar seconddataset = [\"Hello\",\"World\"]\n",
        "\tfunction float2int (value) {\n",
        "\treturn value | 0;\n",
        "\t}\n",
        "\tfunction tohex(fraction)\n",
        "\t{\n",
        "\tvar value = float2int(255 * fraction);\n",
        "\tif(value==0)\n",
        "\treturn \"00\"\n",
        "\tvar mapping = ['A','B','C','D','E','F'];\n",
        "\tvar hex=\"\";\n",
        "\twhile(value!==0)\n",
        "\t{\n",
        "\tvar curr= value%16;\n",
        "\tif(curr >9)\n",
        "\t  {\n",
        "\t    hex = mapping[curr-10] + hex;\n",
        "\t  }\n",
        "\telse\n",
        "\t  hex = curr + hex;\n",
        "\tvalue = float2int(value/16);\n",
        "\t}\n",
        "\treturn hex;\n",
        "\t}\n",
        "\td3.select('#text')\n",
        "\t.selectAll('#text')\n",
        "\t.data($data)\n",
        "\t.enter()\n",
        "\t.append('tspan')\n",
        "\t.style('font-family','verdana')\n",
        "\t$style\n",
        "\t.style('margin','2px')\n",
        "\t.attr(\"class\",\"tooltip\")\n",
        "\t.style('font-size',function(d){return  $offset +$scale*d.weight + \"px\" ;})\n",
        "\t.attr(\"onmouseover\", \"handleMouseOver()\")\n",
        "\t.text(function(d){\n",
        "\treturn d.token+\" \" ;\n",
        "\t})\n",
        "\t.append('span')\n",
        "\t.attr('class',\"tooltiptext\")\n",
        "\t.text(function(d){\n",
        "\treturn Math.round(d.weight*10000)/100;\n",
        "\t});\n",
        "\t''')\n",
        "\n",
        "\tstyle1 =\".style('background-color', function(d,i){return '#FF' +tohex(1-d.weight) +tohex(1-d.weight) ;})\"\n",
        "\tdata = [{'token':token, 'weight' :weight} for token,weight in zip(sentence,attention_weights)]\n",
        "\n",
        "\tjs_text = js_template.substitute({'data' : json.dumps(data),'scale': 0 if style==1 else scale, 'offset':offset, 'style': style1 if style==1 else \"\"})\n",
        "\n",
        "\n",
        "\treturn display(HTML(html_template.substitute({'css_text':css_text,'js_text':js_text})))\n",
        "\n",
        "\n",
        "sentence = ['the',\n",
        " 'food',\n",
        " 'is',\n",
        " 'here',\n",
        " 'is',\n",
        " 'positive',\n",
        " 'though',\n",
        " 'the',\n",
        " 'quality',\n",
        " 'is',\n",
        " 'negative',\n",
        " 'during',\n",
        " 'lunch']\n",
        "attention_weights = [0.00020332, 0.08182091, 0.00613416, 0.00028234, 0.08213249,\n",
        "        0.08816207, 0.08652914, 0.00445198, 0.08577133, 0.08296202,\n",
        "        0.08816148, 0.07696584, 0.08714949]\n",
        "\n",
        "result = display_attention(sentence, attention_weights)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\t<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
              "\t<style>\n",
              "\thtml, body {\n",
              "\tmargin: 0;\n",
              "\tpadding: 0;\n",
              "\t}\n",
              "\t.tooltip {\n",
              "\tposition: relative;\n",
              "\tdisplay: inline-block;\n",
              "\tborder-bottom: 1px dotted black;\n",
              "\t}\n",
              "\t.tooltip:hover .tooltiptext {\n",
              "\tvisibility: visible;\n",
              "\t}\n",
              "\t.tooltip .tooltiptext {\n",
              "\tvisibility: hidden;\n",
              "\tfont-size:15px;\n",
              "\twidth: 60px;\n",
              "\tbackground-color: black;\n",
              "\tcolor: #fff;\n",
              "\ttext-align: center;\n",
              "\tborder-radius: 6px;\n",
              "\tpadding: 1px 0;\n",
              "\tposition: absolute;\n",
              "\tleft: 50%;\n",
              "\tmargin-left: -60px;\n",
              "\t}\n",
              "\t</style>\n",
              "\t<script>\n",
              "\tvar dataset =[[\"This\",0.4],[\"is\",0.3],[\"a\",0.2],[\"text\",0.4]]\n",
              "\tvar seconddataset = [\"Hello\",\"World\"]\n",
              "\tfunction float2int (value) {\n",
              "\treturn value | 0;\n",
              "\t}\n",
              "\tfunction tohex(fraction)\n",
              "\t{\n",
              "\tvar value = float2int(255 * fraction);\n",
              "\tif(value==0)\n",
              "\treturn \"00\"\n",
              "\tvar mapping = ['A','B','C','D','E','F'];\n",
              "\tvar hex=\"\";\n",
              "\twhile(value!==0)\n",
              "\t{\n",
              "\tvar curr= value%16;\n",
              "\tif(curr >9)\n",
              "\t  {\n",
              "\t    hex = mapping[curr-10] + hex;\n",
              "\t  }\n",
              "\telse\n",
              "\t  hex = curr + hex;\n",
              "\tvalue = float2int(value/16);\n",
              "\t}\n",
              "\treturn hex;\n",
              "\t}\n",
              "\td3.select('#text')\n",
              "\t.selectAll('#text')\n",
              "\t.data([{\"token\": \"the\", \"weight\": 0.00020332}, {\"token\": \"food\", \"weight\": 0.08182091}, {\"token\": \"is\", \"weight\": 0.00613416}, {\"token\": \"here\", \"weight\": 0.00028234}, {\"token\": \"is\", \"weight\": 0.08213249}, {\"token\": \"positive\", \"weight\": 0.08816207}, {\"token\": \"though\", \"weight\": 0.08652914}, {\"token\": \"the\", \"weight\": 0.00445198}, {\"token\": \"quality\", \"weight\": 0.08577133}, {\"token\": \"is\", \"weight\": 0.08296202}, {\"token\": \"negative\", \"weight\": 0.08816148}, {\"token\": \"during\", \"weight\": 0.07696584}, {\"token\": \"lunch\", \"weight\": 0.08714949}])\n",
              "\t.enter()\n",
              "\t.append('tspan')\n",
              "\t.style('font-family','verdana')\n",
              "\t.style('background-color', function(d,i){return '#FF' +tohex(1-d.weight) +tohex(1-d.weight) ;})\n",
              "\t.style('margin','2px')\n",
              "\t.attr(\"class\",\"tooltip\")\n",
              "\t.style('font-size',function(d){return  18 +0*d.weight + \"px\" ;})\n",
              "\t.attr(\"onmouseover\", \"handleMouseOver()\")\n",
              "\t.text(function(d){\n",
              "\treturn d.token+\" \" ;\n",
              "\t})\n",
              "\t.append('span')\n",
              "\t.attr('class',\"tooltiptext\")\n",
              "\t.text(function(d){\n",
              "\treturn Math.round(d.weight*10000)/100;\n",
              "\t});\n",
              "\t</script>\n",
              "\t<div id = 'text' style=\"margin-left:50px;\"></div>\n",
              "\t"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tba-B4dkEmbO"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GRU, TimeDistributed, Input, Dropout, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "POSITION_EMBEDDING_DIM = 50\n",
        "MAX_LEN = 80\n",
        "\n",
        "\n",
        "def reduce_dimension(x, length, mask):\n",
        "    res = K.reshape(x, [-1, length])  # (?, 78)\n",
        "    res = K.softmax(res)\n",
        "    res = res * K.cast(mask, dtype='float32')  # (?, 78)\n",
        "    temp = K.sum(res, axis=1, keepdims=True)  # (?, 1)\n",
        "    temp = K.repeat_elements(temp, rep=length, axis=1)  # (?, 78)\n",
        "    return res / temp\n",
        "\n",
        "\n",
        "def reduce_dimension_output_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    assert len(shape) == 3  # only valid for 3D tensors\n",
        "    return [shape[0], shape[1]]\n",
        "\n",
        "\n",
        "def attention(x, dim):\n",
        "    res = K.batch_dot(x[0], x[1], axes=[1, 1])\n",
        "    return K.reshape(res, [-1, dim])\n",
        "\n",
        "\n",
        "def attention_output_shape(input_shape):\n",
        "    shape = list(input_shape[1])\n",
        "    assert len(shape) == 3\n",
        "    return [shape[0], shape[2]]\n",
        "\n",
        "\n",
        "def no_change(input_shape):\n",
        "    return input_shape\n",
        "\n",
        "\n",
        "def liter(x, length):\n",
        "    res = K.repeat(x, length)  # (?, 82, 300)\n",
        "    return res\n",
        "\n",
        "\n",
        "def liter_output_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    return [shape[0], MAX_LEN, shape[1]]\n",
        "\n",
        "\n",
        "def build_model(max_len=80, aspect_max_len=80, embedding_matrix=[],\n",
        "                position_embedding_matrix=[], class_num=3, num_words=3700):\n",
        "\n",
        "    MAX_LEN = max_len\n",
        "    # ============================= Input =================================================\n",
        "    sentence_input = Input(shape=(max_len,), dtype='int32', name='sentence_input')  # (?, 78)\n",
        "    position_input = Input(shape=(max_len,), dtype='int32', name='position_input')  # (?, 78)\n",
        "    aspect_input = Input(shape=(aspect_max_len,), dtype='int32', name='aspect_input')  # (?, 78)\n",
        "\n",
        "    sentence_embedding_layer = Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                         input_length=max_len, trainable=False, mask_zero=True)\n",
        "    sentence_embedding = sentence_embedding_layer(sentence_input)  # (?, 78, 300)\n",
        "\n",
        "    position_embedding = Embedding(max_len * 2, POSITION_EMBEDDING_DIM, weights=[position_embedding_matrix],\n",
        "                                   input_length=max_len, trainable=True, mask_zero=True)(position_input)  # (?, 78, 50)\n",
        "\n",
        "    aspect_embedding_layer = Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                       input_length=aspect_max_len, trainable=False, mask_zero=True)\n",
        "    aspect_embedding = aspect_embedding_layer(aspect_input)  # (?, 9, 300)\n",
        "    # ============================= Input =================================================\n",
        "\n",
        "    # =========================== GRU Layer ===============================================\n",
        "    input_embedding = keras.layers.concatenate([sentence_embedding, position_embedding])  # (?, 78, 350)\n",
        "    encode_x = Bidirectional(GRU(300, activation=\"relu\", return_sequences=True, recurrent_dropout=0.5, dropout=0.5))(input_embedding)  # (?, 78, 600)\n",
        "    # =========================== GRU Layer ===============================================\n",
        "\n",
        "    # =========================== attention ===============================================\n",
        "\n",
        "    # --------------------------- source2aspect attention ----------------------------------------\n",
        "    aspect_attention = TimeDistributed(Dense(1, activation='tanh'))(aspect_embedding)  # (?, 9, 1)\n",
        "    aspect_attention = Lambda(reduce_dimension,\n",
        "                              output_shape=reduce_dimension_output_shape,\n",
        "                              arguments={'length': aspect_max_len},\n",
        "                              mask=aspect_embedding_layer.get_output_mask_at(0),\n",
        "                              name='aspect_attention')(aspect_attention)  # (?, 9)\n",
        "    aspect_embedding = Lambda(attention,\n",
        "                              output_shape=attention_output_shape,\n",
        "                              arguments={'dim': 300})([aspect_attention, aspect_embedding])  # (?, 300)\n",
        "    # --------------------------- aspect attention ----------------------------------------\n",
        "\n",
        "    aspect_embedding = Lambda(liter,\n",
        "                              output_shape=liter_output_shape,\n",
        "                              arguments={'length': max_len})(aspect_embedding)  # (?, 82, 300)\n",
        "    x = keras.layers.concatenate([aspect_embedding, encode_x])  # (?, 82, 900)\n",
        "    x = TimeDistributed(Dense(300, activation='tanh'))(x)   # (?, 82, 300)\n",
        "    x = keras.layers.concatenate([x, encode_x])  # (?, 82, 900)\n",
        "\n",
        "    # --------------------------- source2token attention ----------------------------------------\n",
        "    x = TimeDistributed(Dense(1, activation='tanh'))(x)  # (?, 78, 1)\n",
        "    attention_x = Lambda(reduce_dimension,\n",
        "                         output_shape=reduce_dimension_output_shape,\n",
        "                         arguments={'length': max_len},\n",
        "                         mask=sentence_embedding_layer.get_output_mask_at(0),\n",
        "                         name='attention_x')(x)  # (?, 78)\n",
        "    # --------------------------- source2token attention ----------------------------------\n",
        "\n",
        "    # =========================== attention ===============================================\n",
        "\n",
        "    x = Lambda(attention, output_shape=attention_output_shape, arguments={'dim': 600})([attention_x, encode_x])  # (?, 600)\n",
        "\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    predictions = Dense(class_num, activation='softmax')(x)  # (?, 3)\n",
        "\n",
        "    model = Model(inputs=[sentence_input, position_input, aspect_input], outputs=predictions)\n",
        "    model.compile(loss=['categorical_crossentropy'], optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIlYTQgwOefS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "280811a7-8183-438c-d026-10a3c590d139"
      },
      "source": [
        "def load_position_matrix():\n",
        "        position_matrix_file = \"position_matrix.txt\"\n",
        "        rf = open(position_matrix_file, 'r')\n",
        "        position_matrix = []\n",
        "        while True:\n",
        "            line = rf.readline()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            temp = line.strip().split()\n",
        "            for i in range(len(temp)):\n",
        "                temp[i] = float(temp[i])\n",
        "            position_matrix.append(temp)\n",
        "        rf.close()\n",
        "        position_matrix = np.array(position_matrix)\n",
        "        return position_matrix\n",
        "\n",
        "position_matrix= load_position_matrix()\n",
        "position_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(164, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd9YKbQmEvtm"
      },
      "source": [
        "\n",
        "model = build_model(maxlen, as_max_len, embedding_vector, position_matrix,3, embed_size)\n",
        "model.fit([xtr, xtr_mask], ytr, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}